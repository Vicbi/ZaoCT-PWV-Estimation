{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Utilities.utils import *\n",
    "from Utilities.plot_functions import *\n",
    "# from hyper_tuning import hyperparameter_selection_random_forest_n_estimators, hyperparameter_selection_random_forest_max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Regression Model & Prediction Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select prediction variable, e.g., 'CT_sys', 'Z_compl' an define its unit\n",
    "prediction_variable = 'Z_compl'  \n",
    "# variable_unit = 'mL/mmHg'\n",
    "variable_unit = 'mmHg.s/mL'\n",
    "\n",
    "# Select regression method, e.g., 'RF','ANN'\n",
    "regressor = 'RF'  \n",
    "\n",
    "# Select model based on the input vector\n",
    "# M1 uses as inputs brSBP, brDBP, HR, cfPWV, crPWV.\n",
    "# M2 uses as inputs brSBP, brDBP, cfPWV, crPWV.\n",
    "# M3 uses as inputs MAP, cfPWV, crPWV.\n",
    "# M4 uses as inputs cfPWV, crPWV.\n",
    "model_selection = 'M1'\n",
    "\n",
    "# Enable noise addition and select noise level in %\n",
    "noise_mode = True\n",
    "noise_level = 15\n",
    "\n",
    "# Enable figure saving\n",
    "save_figure_boolean = False \n",
    "\n",
    "# Enable results printing during training\n",
    "verbose = False\n",
    "\n",
    "experiment_type = 'insilico'    # Set the type of data being used in the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "dataset = pd.read_csv('Data/insilico_Zao_CT_dataset.csv')\n",
    "\n",
    "# Add noise to the dataset\n",
    "dataset = add_random_noise(dataset, noise_level, noise_level, noise_mode)\n",
    "\n",
    "print('The dataset size is:', dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = select_features(dataset, model_selection, prediction_variable)\n",
    "\n",
    "# Scale dataset \n",
    "scaled_dataset = scale_data(dataset)\n",
    "\n",
    "X, y = split_features_target(scaled_dataset)\n",
    "\n",
    "indices = np.arange(len(X))\n",
    "X_train_val, X_test, y_train_val, y_test, indices_train_val, indices_test = train_test_split(X, y,indices, test_size=0.20, random_state=42)\n",
    "\n",
    "indices2 = np.arange(len(X_train_val))\n",
    "X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(X_train_val, y_train_val,indices2,test_size=0.25, random_state=42) # 0.25x0.8 = 0.20\n",
    "\n",
    "print('The train set size is:', X_train.shape)\n",
    "print('The test set size is:', X_test.shape)\n",
    "print('The validation set size is:', X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_max_depth = tune_random_forest_max_depth(X_train_val, y_train_val)\n",
    "# optimal_epochs = tune_ann_epochs(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, y_pred = select_regression_model(X_train, X_test, y_train, y_test, prediction_variable, regressor)\n",
    "# model, y_pred, hyper_parameters = hyperparameter_tuning(X_train, X_test,y_train, y_test,regressor)\n",
    "\n",
    "    \n",
    "y_test_scaled = rescale_values(y_test, prediction_variable, dataset)\n",
    "y_pred_scaled = rescale_values(y_pred, prediction_variable, dataset)\n",
    "\n",
    "if regressor == 'ANN':\n",
    "    y_pred_scaled = np.ravel(y_pred_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(y_test_scaled, y_pred_scaled, variable_unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(y_test_scaled,y_pred_scaled,experiment_type,prediction_variable,regressor,save_figure_boolean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model, y_pred = select_regression_model(X_train, X_test, y_train, y_test, prediction_variable, regressor)\n",
    "\n",
    "# Define parameters\n",
    "num_iterations = 20\n",
    "rmse_differences = []\n",
    "importance_sum = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Run the permutation feature importance calculation multiple times\n",
    "for _ in range(num_iterations):\n",
    "    importances = permutation_importances(model, X_train, y_train, mean_squared_error)\n",
    "    baseline_rmse = mean_squared_error(y_train, model.predict(X_train))\n",
    "    new_rmse = baseline_rmse + importances\n",
    "    rmse_difference = new_rmse - baseline_rmse\n",
    "    rmse_differences.append(rmse_difference)\n",
    "    importance_sum += importances\n",
    "\n",
    "# Calculate mean and standard deviation of RMSE differences\n",
    "mean_rmse_difference = np.mean(rmse_differences)\n",
    "std_dev_rmse_difference = np.std(rmse_differences)\n",
    "\n",
    "# Calculate average importance\n",
    "average_importance = importance_sum / num_iterations\n",
    "\n",
    "# Create a list of tuples with feature index and average importance\n",
    "feature_importance_tuples = list(enumerate(average_importance))\n",
    "\n",
    "# Sort the list by importance in ascending order\n",
    "feature_importance_tuples.sort(key=lambda x: x[1])\n",
    "\n",
    "# Print the importances in ascending order\n",
    "for i, importance in feature_importance_tuples:\n",
    "    print(f'Feature {i+1}: {importance}')\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nMean Increase in RMSE: {np.round(mean_rmse_difference,6)}\")\n",
    "print(f\"Standard Deviation of Increase in RMSE: {np.round(std_dev_rmse_difference,6)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
